{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-975575f7b0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# open file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmusdb18_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'musdb_classify_tcontext_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#with h5py.File('/media/archive/' + 'musdb_classify_tcontext_' + str(tcontext) + '.hdf5', 'w') as hdf:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#store hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch1/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch1/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')"
     ]
    }
   ],
   "source": [
    "import musdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import librosa as lsa\n",
    "\n",
    "musdb18_path = '/home/enricguso/datasets/musdb18/' #path to the musdb folder.\n",
    "mus = musdb.DB(root_dir=musdb18_path)\n",
    "fft_size = 1024\n",
    "\n",
    "tcontext = 120 #amount of temporal frames considered by the model\n",
    "\n",
    "\n",
    "# load the training tracks pointers\n",
    "train_tracks = mus.load_mus_tracks(subsets=['train'])\n",
    "    \n",
    "# load the test tracks pointers\n",
    "test_tracks = mus.load_mus_tracks(subsets=['test'])\n",
    "\n",
    "# open file\n",
    "with h5py.File(musdb18_path + 'musdb_classify_tcontext_' + str(tcontext) + '.hdf5', 'w') as hdf: \n",
    "#with h5py.File('/media/archive/' + 'musdb_classify_tcontext_' + str(tcontext) + '.hdf5', 'w') as hdf: \n",
    "    #store hyperparameters\n",
    "    hdf.attrs.create('tcontext', tcontext)\n",
    "    #initialize frequency index variable\n",
    "    f_starts = np.array([], dtype='intp')\n",
    "    f_starts = np.hstack((f_starts, (0)))\n",
    "    # We start two counters: for each file and for each STFT frame\n",
    "    file_ind = 0\n",
    "    n = 0\n",
    "    #initialize variables for stats\n",
    "    mean = np.zeros(int(fft_size / 2 + 1), dtype=np.float32)\n",
    "    M2 = np.zeros(int(fft_size / 2 + 1), dtype=np.float32)\n",
    "    \n",
    "    #initialize inputs and labels datasets. Uses chunking based on tcontext\n",
    "    hdf.create_dataset('track_mag', (1, 1), maxshape=(None, int(fft_size / 2 + 1)),\n",
    "                       chunks=(tcontext, int(fft_size / 2 + 1)), dtype='f')\n",
    "    hdf.create_dataset('label', (1, 1), maxshape=(None, 1), chunks=True, dtype=i1)\n",
    "    #Computed with the online welford algorithm\n",
    "    hdf.create_dataset('mean', (1, int(fft_size / 2 + 1)), 'f')  # binwise statics\n",
    "    hdf.create_dataset('std', (1, int(fft_size / 2 + 1)), 'f')\n",
    "    \n",
    "    #Define insert function for each track\n",
    "    def insert_track(track, tcontext, n,  mean, M2, f_starts, label, file_ind):\n",
    "        #Cast to double precision\n",
    "        track = np.float32(track)\n",
    "        # Find out how much padding is needed so STFS fit tcontext hdf5-chunking\n",
    "        padding = len(track)\n",
    "        while ((padding / fft_size) * int(fft_size / int(fft_size/4)) + 1) % tcontext != 0:\n",
    "            padding += 1\n",
    "        track = np.hstack((track, np.zeros((padding - len(track)), dtype=np.float32)))\n",
    "        \n",
    "        #compute STFT using librosa\n",
    "        track = np.abs(\n",
    "            lsa.core.stft(track, n_fft=fft_size, hop_length=int(fft_size/4), win_length=None, window='hann',\n",
    "                          center=True, dtype='complex64', pad_mode='reflect'))\n",
    "        track = np.swapaxes(track, 0, 1)\n",
    "\n",
    "        # Store input and label in HDF5\n",
    "        f_starts = np.hstack((f_starts, (len(track) + f_starts[-1])))  # we store frequency index\n",
    "        hdf['track_mag'].resize((f_starts[-1], int(fft_size / 2 + 1)))\n",
    "        hdf['track_mag'][f_starts[-2]:f_starts[-1], :] = track\n",
    "        hdf['label'].resize((file_ind + 1, 1))\n",
    "        hdf['label'][file_ind, 0] = label\n",
    "        \n",
    "        file_ind +=1\n",
    "\n",
    "        # Then do online STD using welford's algorithm\n",
    "        for timeframe in range(len(track)):\n",
    "            # Iterate over time STFT\n",
    "            n += 1\n",
    "            delta = track[timeframe, :] - mean\n",
    "            mean = mean + delta / n\n",
    "            M2 = M2 + delta * (track[timeframe, :] - mean)\n",
    "\n",
    "        return n, mean, M2, f_starts, file_ind\n",
    "    \n",
    "    #Run for each track in the dataset: get audio, downmix to mono, insert to dataset\n",
    "    for track in train_tracks:\n",
    "        aux=track.targets['vocals'].audio\n",
    "        vocals=0.5*aux[:,0]+0.5*aux[:,1]\n",
    "        n,  mean, M2, f_starts, file_ind = insert_track(vocals, tcontext, n,  mean, M2, f_starts, 0, file_ind)\n",
    "        aux=track.targets['drums'].audio\n",
    "        drums=0.5*aux[:,0]+0.5*aux[:,1]\n",
    "        n,  mean, M2, f_starts, file_ind = insert_track(drums, tcontext, n,  mean, M2, f_starts, 1, file_ind)\n",
    "        aux=track.targets['bass'].audio\n",
    "        bass=0.5*aux[:,0]+0.5*aux[:,1]\n",
    "        n,  mean, M2, f_starts, file_ind = insert_track(bass, tcontext, n,  mean, M2, f_starts, 2, file_ind)\n",
    "        aux=track.targets['other'].audio\n",
    "        other=0.5*aux[:,0]+0.5*aux[:,1]\n",
    "        n,  mean, M2, f_starts, file_ind = insert_track(other, tcontext, n,  mean, M2, f_starts, 3, file_ind)\n",
    "        \n",
    "        if file_ind % 5 == 0:\n",
    "            print(str((len(test_tracks)+len(train_tracks))*4 - (file_ind)) + ' files remaining...')\n",
    "    \n",
    "    for track in test_tracks:\n",
    "        aux=track.targets['vocals'].audio\n",
    "        vocals=0.5*aux[:,0]+0.5*aux[:,1]\n",
    "        n,  mean, M2, f_starts, file_ind = insert_track(vocals, tcontext, n,  mean, M2, f_starts, 0, file_ind)\n",
    "        aux=track.targets['drums'].audio\n",
    "        drums=0.5*aux[:,0]+0.5*aux[:,1]\n",
    "        n,  mean, M2, f_starts, file_ind = insert_track(drums, tcontext, n,  mean, M2, f_starts, 1, file_ind)\n",
    "        aux=track.targets['bass'].audio\n",
    "        bass=0.5*aux[:,0]+0.5*aux[:,1]\n",
    "        n,  mean, M2, f_starts, file_ind = insert_track(bass, tcontext, n,  mean, M2, f_starts, 2, file_ind)\n",
    "        aux=track.targets['other'].audio\n",
    "        other=0.5*aux[:,0]+0.5*aux[:,1]\n",
    "        n,  mean, M2, f_starts, file_ind = insert_track(other, tcontext, n,  mean, M2, f_starts, 3, file_ind)\n",
    "        \n",
    "        if file_ind % 5 == 0:\n",
    "            print(str((len(test_tracks)+len(train_tracks))*4 - (file_ind)) + ' files remaining...')\n",
    "            \n",
    "    # Finally store statics in HDF5\n",
    "    hdf['mean'][0, ...] = mean\n",
    "    hdf['std'][0, ...] = np.sqrt(M2/ (n - 1))\n",
    "\n",
    "    # Also store indexes of frequency data streams: index corresponds to utterance starting point\n",
    "    hdf.create_dataset('f_indexes', (len(f_starts),), dtype='intp')\n",
    "    hdf['f_indexes'][...] = f_starts\n",
    "    # Close the HDF5 file\n",
    "    hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
